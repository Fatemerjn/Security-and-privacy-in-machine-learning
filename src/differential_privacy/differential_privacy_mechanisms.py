"""
Auto-generated from notebooks/differential_privacy_mechanisms.ipynb.

Generated by tools/convert_notebooks.py. Do not edit manually.
"""


def main():
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.datasets import fetch_openml # need sklearn >= 0.22
    import scipy as sp
    from sklearn.model_selection import train_test_split, KFold
    from sklearn.preprocessing import Normalizer, StandardScaler
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import ParameterGrid
    from sklearn.svm import SVC
    from sklearn.metrics import log_loss
    from sklearn.preprocessing import LabelBinarizer


    def bar_plot_pandas(series1, series2=None, label1="Series 1", label2="Series 2", title=""):
        '''
        Draws a bar plot of one Pandas Series, or two pandas Series with the same index

        Parameters
        ----------
        series1 : Series of float
            First input
        series2 : Series of float, optional
            Second input (with same index)
        label1 : string, optional
            Label for the first series
        label2 : string, optional
            Label for the second series
        title : string, optional
            Plot title
        '''
        if series2 is None:
            series1.plot.bar()
            plt.xlabel("Index")
            plt.ylabel("Values")
            plt.title(title if title else "Bar Plot")
            plt.legend([label1])
            plt.show()
        else:
            # Create a DataFrame directly from the two series, aligning by index
            df = pd.DataFrame({label1: series1, label2: series2})
            df.plot.bar()
            plt.xlabel("Index")
            plt.ylabel("Values")
            plt.title(title if title else "Comparison Bar Plot")
            plt.legend()
            plt.show()


    # Fetching the Adult dataset from OpenML
    dataset_handle = fetch_openml(name='adult', version=2, as_frame=True)
    dataset = dataset_handle.frame


    # Exploring the dataset
    n, d = dataset.shape  # Number of rows and columns
    print(f"Dataset contains {n} rows (individuals) and {d} columns (attributes).")
    print("\nPreview of the dataset:")
    print(dataset.head(4))


    def count_query(df, attribute, value):
        '''
        Parameters
        ----------
        df : DataFrame
            Dataset
        attribute : string
            Name of an attribute with categorical values
        value : string or int
            Value of attribute to count

        Returns
        -------
        count : int
            The number of records with `attribute=value` in dataset `df`
        '''
        # Count the number of rows where df[attribute] equals the specified value
        return (df[attribute] == value).sum()


    def average_query(df, attribute):
        '''
        Parameters
        ----------
        df : DataFrame
            Dataset
        attribute : string
            Name of an attribute with numeric values

        Returns
        -------
        average : float
            The average value of `attribute` in dataset `df`
        '''
        # Compute the mean of the specified numeric column
        return df[attribute].mean()


    def histogram_query(df, attribute):
        '''
        Parameters
        ----------
        df : DataFrame
            Dataset
        attribute : string
            Name of an attribute with categorical values

        Returns
        -------
        histogram : array or Series of int
            The histogram of `attribute`, i.e., the number of times each value of `attribute` appears in `df`
        '''
        # Use value_counts to get frequency of each categorical value in the column
        return df[attribute].value_counts()


    def laplace_mechanism(q, s1, eps, random_state=None):
        '''
        Parameters
        ----------
        q : float or array/Series of float
            The non-private output of the query
        s1 : float
            The L1 sensitivity of the query
        eps : float
            Parameter epsilon of differential privacy
        random_state : int, optional (default=None)
            Random seed

        Returns
        -------
        private_q : float or array/Series of float
            An eps-DP evaluation of the query
        '''
        rng = np.random.RandomState(random_state)

        # Determine the scale for Laplace noise
        scale = s1 / eps

        if hasattr(q, 'shape'):  # query output is multi-dimensional (e.g., array or Series)
            # Create noise with the same shape as q
            noise = rng.laplace(loc=0.0, scale=scale, size=q.shape)

            # Add noise to the original query output
            private_q = q + noise

            # If q is a pandas Series, ensure the result retains the same index
            if isinstance(q, pd.Series):
                private_q = pd.Series(private_q, index=q.index)
        else:  # query output is a scalar
            noise = rng.laplace(loc=0.0, scale=scale)
            private_q = q + noise

        return private_q


    def l1_error(a, b):
        '''
        Parameters
        ----------
        a : float or array/Series of float
            First input
        b : float or array/Series of float
            Second input

        Returns
        -------
        l1_error : float
            The L1 distance between `a` and `b`: ||a-b||_1
        '''
        a_arr = np.asarray(a)
        b_arr = np.asarray(b)
        return np.sum(np.abs(a_arr - b_arr))

    # Define the queries with their sensitivities
    queries = [
        # Query for counting number of males
        (lambda df: count_query(df, "sex", "Male"), 1),

        # Query for histogram of "workclass"
        (lambda df: histogram_query(df, "workclass"), 1)
    ]

    for eps in [0.1, 1.0]:
        print(f"\n--- Results for ε = {eps} ---")
        for query_func, sensitivity in queries:
            true_result = query_func(dataset)
            private_result = laplace_mechanism(true_result, sensitivity, eps, random_state=42)

            error_value = l1_error(private_result, true_result)

            if isinstance(true_result, pd.Series):
                print(f"\nHistogram for ε = {eps}:")
                print("True histogram:\n", true_result)
                print("Private histogram (rounded):\n", private_result.round())
                print(f"L1 error: {error_value}")
                bar_plot_pandas(true_result, private_result.round(),
                                label1="True", label2="Private",
                                title=f"Workclass Histogram (ε={eps})")
            else:
                print(f"\nCount for ε = {eps}:")
                print(f"True count: {true_result}, Private count: {private_result}, L1 error: {error_value}")


    def gaussian_mechanism(q, s2, eps, delta, random_state=None):
        '''
        Parameters
        ----------
        q : float or array/Series of float
            The non-private output of the query
        s2 : float
            The L2 sensitivity of the query
        eps : float
            Parameter epsilon of differential privacy
        delta : float
            Parameter delta of differential privacy
        random_state : int, optional (default=None)
            Random seed

        Returns
        -------
        private_q : float or array/Series of float
            An (eps,delta)-DP evaluation of the query
        '''

        rng = np.random.RandomState(random_state)

        # Calculate the standard deviation for the Gaussian noise
        sigma = (s2 * np.sqrt(2 * np.log(1.25 / delta))) / eps

        if hasattr(q, 'shape'):  # query output is multi-dimensional
            noise = rng.normal(loc=0.0, scale=sigma, size=q.shape)
            private_q = q + noise
            # If q is a pandas Series, maintain the index
            if isinstance(q, pd.Series):
                private_q = pd.Series(private_q, index=q.index)
        else:  # query output is a scalar
            noise = rng.normal(loc=0.0, scale=sigma)
            private_q = q + noise

        return private_q


    # Define the 1D query: average age
    q_value = average_query(dataset, "age")

    # Sensitivities for the average query
    # For a count query, sensitivity (s1) was 1 because a change in one individual's presence changes the sum by at most the maximum value.
    # For the average query over n individuals, sensitivity = (max - min) / n.
    s1 = (dataset["age"].max() - dataset["age"].min()) / len(dataset)

    # For Gaussian mechanism, we need an L2 sensitivity (s2).
    # For a 1D output like the average, the L2 sensitivity can be considered the same as the L1 sensitivity.
    s2 = s1

    eps_list = np.linspace(0.01, 10.0, num=20)
    delta1 = 1. / len(dataset)**2
    delta2 = 1. / len(dataset)**4
    n_runs = 50

    error = np.zeros((len(eps_list), 3, n_runs))

    for i, eps in enumerate(eps_list):
        for r in range(n_runs):
            # Laplace mechanism error
            lap_result = laplace_mechanism(q_value, s1, eps, random_state=r)
            error[i, 0, r] = l1_error(lap_result, q_value)

            # Gaussian mechanism error for delta1
            gauss1 = gaussian_mechanism(q_value, s2, eps, delta1, random_state=r)
            error[i, 1, r] = l1_error(gauss1, q_value)

            # Gaussian mechanism error for delta2
            gauss2 = gaussian_mechanism(q_value, s2, eps, delta2, random_state=r)
            error[i, 2, r] = l1_error(gauss2, q_value)

    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    ax.errorbar(eps_list, error[:, 0, :].mean(axis=1), yerr=error[:, 0, :].std(axis=1),
                label='Laplace mechanism')
    ax.errorbar(eps_list, error[:, 1, :].mean(axis=1), yerr=error[:, 1, :].std(axis=1),
                label='Gaussian mechanism (δ=' + "{:.2e}".format(delta1) + ')')
    ax.errorbar(eps_list, error[:, 2, :].mean(axis=1), yerr=error[:, 2, :].std(axis=1),
                label='Gaussian mechanism (δ=' + "{:.2e}".format(delta2) + ')')
    plt.xlabel("ε")
    plt.ylabel("ℓ₁ error")
    ax.set_yscale('log')
    ax.legend()
    plt.show()


    def table_query(df, row_attr_list, col_attr_list):
        '''
        Parameters
        ----------
        df : DataFrame
            Dataset
        row_attr_list : list
            List of attributes to use as rows
        col_attr_list : list
            List of attributes to use as columns

        Returns
        -------
        cross_table : DataFrame
            Cross tabulation (contingency table) of `df` according to `row_attr_list` and `col_attr_list`
        '''
        # Use pandas.crosstab to compute the contingency table
        cross_table = pd.crosstab(index=[df[attr] for attr in row_attr_list],
                                  columns=[df[attr] for attr in col_attr_list])
        return cross_table


    def private_average_age_with_clipping(df, attribute="age", lower_bound=0, upper_bound=100, eps=1.0, random_state=None):
        """
        Computes a differentially private average of an attribute after clipping its values.

        Parameters
        ----------
        df : DataFrame
            The dataset containing the attribute.
        attribute : string
            The name of the numeric attribute to average.
        lower_bound : float
            The lower bound to clip the attribute values.
        upper_bound : float
            The upper bound to clip the attribute values.
        eps : float
            Differential privacy parameter epsilon.
        random_state : int, optional
            Random seed for reproducibility.

        Returns
        -------
        private_avg : float
            The differentially private average of the clipped attribute.
        """
        # Clip the attribute values between lower_bound and upper_bound
        clipped_values = df[attribute].clip(lower=lower_bound, upper=upper_bound)

        # Compute the average of the clipped values
        avg_value = clipped_values.mean()

        # Number of individuals in the dataset
        n = len(clipped_values)

        # Sensitivity after clipping
        sensitivity = (upper_bound - lower_bound) / n

        # Apply Laplace mechanism to the average
        private_avg = laplace_mechanism(avg_value, sensitivity, eps, random_state=random_state)
        return private_avg

    # Example usage:
    eps = 1.0
    private_avg_age = private_average_age_with_clipping(dataset, attribute="age", lower_bound=0, upper_bound=100, eps=eps, random_state=42)
    print("Private average age:", private_avg_age)


    def private_average_age_divorced(df, attribute="age", marital_status="Divorced",
                                   lower_bound=0, upper_bound=100, eps=1.0, random_state=None):
        """
        Computes a differentially private average age for individuals with a specific marital status (e.g., Divorced)
        after clipping age values.

        Parameters
        ----------
        df : DataFrame
            The dataset containing the attributes.
        attribute : string
            The numeric attribute to average (e.g., "age").
        marital_status : string
            The marital status to filter on (e.g., "Divorced").
        lower_bound : float
            The lower bound to clip the age values.
        upper_bound : float
            The upper bound to clip the age values.
        eps : float
            Differential privacy parameter epsilon.
        random_state : int, optional
            Random seed for reproducibility.

        Returns
        -------
        private_avg : float
            The differentially private average age for the specified group.
        """
        # Filter the dataset for individuals with the specified marital status
        subset = df[df["marital-status"] == marital_status]

        # Clip the age values in the subset between lower_bound and upper_bound
        clipped_values = subset[attribute].clip(lower=lower_bound, upper=upper_bound)

        # Compute the average of the clipped ages
        avg_value = clipped_values.mean()

        # Number of individuals in the subset
        n_divorced = len(clipped_values)
        if n_divorced == 0:
            raise ValueError(f"No individuals found with marital status {marital_status}.")

        # Sensitivity for the average over the subset
        sensitivity = (upper_bound - lower_bound) / n_divorced

        # Apply Laplace mechanism to the average
        private_avg = laplace_mechanism(avg_value, sensitivity, eps, random_state=random_state)
        return private_avg

    # Example usage:
    eps = 1.0
    private_avg_divorced_age = private_average_age_divorced(dataset, attribute="age",
                                                            marital_status="Divorced",
                                                            lower_bound=0, upper_bound=100,
                                                            eps=eps, random_state=42)
    print("Private average age of divorced people:", private_avg_divorced_age)


    def sgd(X, y, gamma, n_iter, obj_and_grad, theta_init, n_batch=1, freq_obj_eval=10,
            n_obj_eval=1000, random_state=None):
        """Stochastic Gradient Descent (SGD) algorithm

        Parameters
        ----------
        X : array, shape (n, d)
            The data
        y : array, shape (n,)
            Binary labels (-1, 1).
        gamma : float | callable
            The step size. Can be a constant float or a function
            that allows to have a variable step size
        n_iter : int
            The number of iterations
        obj_and_grad : callable
            A function which takes as a vector of shape (p,), a dataset of shape (n_batch, d)
            and a label vector of shape (n_batch,), and returns the objective value and gradient.
        theta_init : array, shape (p,)
            The initial value for the model parameters
        n_batch : int
            Size of the mini-batch to use at each iteration of SGD.
        freq_obj_eval : int
            Specifies the frequency (in number of iterations) at which we compute the objective
        n_obj_eval : int
            The number of points on which we evaluate the objective
        random_state : int
            Random seed to make the algorithm deterministic


        Returns
        -------
        theta : array, shape=(p,)
            The final value of the model parameters
        obj_list : list of length (n_iter / freq_obj_eval)
            A list containing the value of the objective function computed every freq_obj_eval iterations
        """

        rng = np.random.RandomState(random_state)
        n, d = X.shape
        p = theta_init.shape[0]

        theta = theta_init.copy()

        # if a constant step size was provided, we turn it into a constant function
        if not callable(gamma):
            def gamma_func(t):
                return gamma
        else:
            gamma_func = gamma

        # list to record the evolution of the objective (for plotting)
        obj_list = []
        # we draw a fixed subset of points to monitor the objective
        idx_eval = rng.randint(0, n, n_obj_eval)

        for t in range(n_iter):
            # Evaluate the objective on a fixed subset every freq_obj_eval iterations
            if t % freq_obj_eval == 0:
                obj, _ = obj_and_grad(theta, X[idx_eval, :], y[idx_eval])
                obj_list.append(obj)

            # 1. Sample a mini-batch of size n_batch
            idx_batch = rng.randint(0, n, n_batch)
            X_batch = X[idx_batch, :]
            y_batch = y[idx_batch]

            # 2. Compute gradient on the mini-batch
            _, grad = obj_and_grad(theta, X_batch, y_batch)

            # 3. Compute step size
            step_size = gamma_func(t)

            # 4. Gradient descent update
            theta -= step_size * grad

        return theta, obj_list


    from sklearn.linear_model._base import LinearClassifierMixin, SparseCoefMixin, BaseEstimator
    from sklearn.utils.extmath import safe_sparse_dot
    from scipy.special import expit
    from sklearn.utils.validation import check_X_y
    from scipy.special import logsumexp

    import numpy as np

    def _intercept_dot(theta, X, y):
        """Compute the dot product of X with the weights and add the intercept if present.

        Parameters
        ----------
        theta : array, shape (d,) or (d+1,)
            The weights and optionally the intercept term.
        X : array, shape (n_samples, n_features)
            Feature matrix.
        y : array, shape (n_samples,)
            Labels (used for sign adjustment).

        Returns
        -------
        w : array, shape (n_features,)
            Weights without intercept.
        c : float
            Intercept term.
        yz : array, shape (n_samples,)
            Dot product of (X @ w + c) multiplied by y.
        """
        n_features = X.shape[1]
        w = theta[:n_features]  # Extract weights
        c = theta[n_features] if len(theta) > n_features else 0  # Extract intercept if present
        yz = y * (np.dot(X, w) + c)  # Compute yz as y * (X @ w + c)
        return w, c, yz

    def log_logistic(z):
        return -logsumexp([np.zeros_like(z), -z], axis=0)

    def my_logistic_obj_and_grad(theta, X, y, lamb):
        """Computes the value and gradient of the objective function of logistic regression defined as:
        min (1/n) \sum_i log_loss(theta;X[i,:],y[i]) + (lamb / 2) \|w\|^2,
        where theta = w (if no intercept), or theta = [w b] (if intercept)

        Parameters
        ----------
        theta_init : array, shape (d,) or (d+1,)
            The initial value for the model parameters. When an intercept is used, it corresponds to the last entry
        X : array, shape (n, d)
            The data
        y : array, shape (n,)
            Binary labels (-1, 1)
        lamb : float
            The L2 regularization parameter


        Returns
        -------
        obj : float
            The value of the objective function
        grad : array, shape (d,) or (d+1,)
            The gradient of the objective function
        """
        n_samples, n_features = X.shape
        grad = np.empty_like(theta)

        w, c, yz = _intercept_dot(theta, X, y)

        # Logistic loss is the negative of the log of the logistic function
        obj = -np.mean(log_logistic(yz)) + .5 * lamb * np.dot(w, w)

        z = expit(yz)
        z0 = (z - 1) * y

        grad[:n_features] = safe_sparse_dot(X.T, z0) / n_samples + lamb * w

        # Case where we fit the intercept
        if grad.shape[0] > n_features:
            grad[-1] = z0.sum() / n_samples
        return obj, grad


    class MySGDLogisticRegression(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):
        """Our own sklearn estimator for logistic regression defined as:
        min (1/n) \sum_i log_loss(theta;X[i,:],y[i]) + (lamb / 2) \|w\|^2,
        where theta = [w b]

        Parameters
        ----------
        gamma : float | callable
            The step size. Can be a constant float or a function
            that allows to have a variable step size
        n_iter : int
            The number of iterations
        lamb : float
            The L2 regularization parameter
        n_batch : int
            Size of the mini-batch to use at each iteration of SGD.
        freq_obj_eval : int
            Specifies the frequency (in number of iterations) at which we compute the objective
        n_obj_eval : int
            The number of points on which we evaluate the objectuve
        random_state : int
            Random seed to make the algorithm deterministic

        Attributes
        ----------
        coef_ : (p,)
            The weights of the logistic regression model.
        intercept_ : (1,)
            The intercept term of the logistic regression model.
        obj_list_: list of length (n_iter / freq_obj_eval)
            A list containing the value of the objective function computed every freq_loss_eval iterations
        """

        def __init__(self, gamma, n_iter, lamb=0, n_batch=1, freq_obj_eval=10, n_obj_eval=1000, random_state=None):
            self.gamma = gamma
            self.n_iter = n_iter
            self.lamb = lamb
            self.n_batch = n_batch
            self.freq_obj_eval = freq_obj_eval
            self.n_obj_eval = n_obj_eval
            self.random_state = random_state

        def fit(self, X, y):

            # WARNING: assumes labels are -1, 1
            X, y = check_X_y(X, y, accept_sparse='csr', dtype=[np.float64, np.float32], order="C")
            self.classes_ = np.unique(y)

            p = X.shape[1]
            theta_init = np.zeros(p+1) # initialize parameters to zero
            # define the function for value and gradient needed by SGD
            obj_grad = lambda theta, X, y: my_logistic_obj_and_grad(theta, X, y, lamb=self.lamb)
            theta, obj_list = sgd(X, y, self.gamma, self.n_iter, obj_grad, theta_init, self.n_batch,
                                  self.freq_obj_eval, self.n_obj_eval, self.random_state)

            # save the learned model into the appropriate quantities used by sklearn
            self.intercept_ = np.expand_dims(theta[-1], axis=0)
            self.coef_ = np.expand_dims(theta[:-1], axis=0)

            # also save list of objective values during optimization for plotting
            self.obj_list_ = obj_list

            return self


    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    import pandas as pd
    import numpy as np

    # Assuming 'dataset' is the DataFrame loaded from OpenML as before
    # Let's assume the target column is named 'class', which we will binarize

    # Separate features and target
    X = dataset.drop(columns=['class'])
    y = dataset['class']

    # Convert target to binary labels: for instance, '>50K' to 1 and '<=50K' to -1
    y = y.apply(lambda x: 1 if x == '>50K' else -1)

    # For simplicity, let's perform one-hot encoding on categorical features
    X_encoded = pd.get_dummies(X)

    # Convert the DataFrame to NumPy arrays
    X_array = X_encoded.values.astype(np.float64)
    y_array = y.values.astype(np.float64)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.2, random_state=42)

    # Now we can use MySGDLogisticRegression on this data
    mlr = MySGDLogisticRegression(gamma=0.05, n_iter=5000, lamb=0, n_batch=1, random_state=42)
    mlr.fit(X_train, y_train)
    print("Test accuracy:", mlr.score(X_test, y_test))

    # Plot the objective function over iterations
    import matplotlib.pyplot as plt
    obj_list = mlr.obj_list_
    iter_list = np.arange(len(obj_list)) * mlr.freq_obj_eval
    plt.plot(iter_list, obj_list)
    plt.xlabel("Iteration")
    plt.ylabel("Objective function")
    plt.title("SGD Objective over Iterations")
    plt.show()


    def private_sgd(X, y, gamma, n_iter, obj_and_grad, theta_init, sigma,
                    n_batch=1, freq_obj_eval=10, n_obj_eval=1000, random_state=None):
        """
        Differentially Private SGD with Gaussian noise addition.

        Parameters
        ----------
        X : array, shape (n, d)
            The data
        y : array, shape (n,)
            Binary labels (-1, 1).
        gamma : float | callable
            Step size.
        n_iter : int
            Number of iterations.
        obj_and_grad : callable
            Function that computes objective and gradient.
        theta_init : array, shape (p,)
            Initial parameters.
        sigma : float
            Standard deviation of Gaussian noise added to gradients.
        n_batch : int
            Mini-batch size (should be 1 for DP-SGD as per specification).
        freq_obj_eval : int
            Frequency of objective evaluation.
        n_obj_eval : int
            Number of points used for objective evaluation.
        random_state : int
            Random seed for reproducibility.

        Returns
        -------
        theta : array, shape=(p,)
            Final model parameters.
        obj_list : list
            List of objective values recorded during training.
        """
        rng = np.random.RandomState(random_state)
        n, d = X.shape
        p = theta_init.shape[0]

        theta = theta_init.copy()

        # Wrap gamma if it's not callable
        if not callable(gamma):
            def gamma_func(t):
                return gamma
        else:
            gamma_func = gamma

        obj_list = []
        idx_eval = rng.randint(0, n, n_obj_eval)

        for t in range(n_iter):
            if t % freq_obj_eval == 0:
                obj, _ = obj_and_grad(theta, X[idx_eval, :], y[idx_eval])
                obj_list.append(obj)

            # For DP-SGD, we enforce n_batch = 1 as specified.
            # Sample one data point
            idx = rng.randint(0, n)
            X_batch = X[idx:idx+1, :]
            y_batch = y[idx:idx+1]

            # Compute gradient on the single data point
            _, grad = obj_and_grad(theta, X_batch, y_batch)

            # Add Gaussian noise to the gradient
            noise = rng.normal(loc=0.0, scale=sigma, size=grad.shape)
            noisy_grad = grad + noise

            # Update step size
            step_size = gamma_func(t)

            # Parameter update
            theta -= step_size * noisy_grad

        return theta, obj_list


    class MyPrivateSGDLogisticRegression(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):
        """Sklearn estimator for logistic regression using differentially private SGD.

        Parameters
        ----------
        gamma : float | callable
            Step size.
        n_iter : int
            Number of iterations.
        sigma : float
            Standard deviation of the Gaussian noise for DP.
        n_batch : int
            Mini-batch size (should be 1 for DP-SGD).
        freq_obj_eval : int
            Frequency of objective evaluation.
        n_obj_eval : int
            Number of points for objective evaluation.
        random_state : int
            Random seed.
        """

        def __init__(self, gamma, n_iter, sigma, n_batch=1, freq_obj_eval=10, n_obj_eval=1000, random_state=None):
            self.gamma = gamma
            self.n_iter = n_iter
            self.sigma = sigma
            self.n_batch = n_batch
            self.freq_obj_eval = freq_obj_eval
            self.n_obj_eval = n_obj_eval
            self.random_state = random_state

        def fit(self, X, y):
            # Validate and preprocess X, y as before
            X, y = check_X_y(X, y, accept_sparse='csr', dtype=[np.float64, np.float32], order="C")
            self.classes_ = np.unique(y)

            p = X.shape[1]
            theta_init = np.zeros(p+1)  # initialize parameters to zero

            # Define the objective and gradient function with regularization lambda = 0
            obj_grad = lambda theta, X_batch, y_batch: my_logistic_obj_and_grad(theta, X_batch, y_batch, lamb=0)

            # Call private_sgd instead of sgd
            theta, obj_list = private_sgd(
                X, y, self.gamma, self.n_iter, obj_grad, theta_init, self.sigma,
                n_batch=self.n_batch, freq_obj_eval=self.freq_obj_eval,
                n_obj_eval=self.n_obj_eval, random_state=self.random_state
            )

            # Save the learned model
            self.intercept_ = np.expand_dims(theta[-1], axis=0)
            self.coef_ = np.expand_dims(theta[:-1], axis=0)
            self.obj_list_ = obj_list

            return self


    # Example parameters (you would adjust these based on your privacy/utility trade-offs)
    gamma = 0.05
    n_iter = 5000
    sigma = 1.0  # Example noise level, should be set based on desired (ε, δ)
    n_batch = 1
    random_state = 42

    private_lr = MyPrivateSGDLogisticRegression(gamma=gamma, n_iter=n_iter, sigma=sigma,
                                                n_batch=n_batch, random_state=random_state)
    private_lr.fit(X_train, y_train)
    print("Private Test accuracy:", private_lr.score(X_test, y_test))

    # Plot objective function trajectory
    plt.plot(np.arange(len(private_lr.obj_list_)) * private_lr.freq_obj_eval, private_lr.obj_list_)
    plt.xlabel("Iteration")
    plt.ylabel("Objective function")
    plt.title("Private SGD Objective over Iterations")
    plt.show()


    X, y = fetch_openml(name='a9a', version=1, return_X_y=True, as_frame=False)
    normalizer = Normalizer()
    X = normalizer.transform(X)
    m, d = X.shape
    print(m, d)


    n = 5
    features = {}
    labels = {}
    for i, idx in enumerate(KFold(n_splits=n, shuffle=True).split(X)):
        features[i] = X[idx[1],:]
        labels[i] = y[idx[1]]

    for i in range(n):
        print("Dataset of participant " + str(i) + ":", features[i].shape, labels[i].shape)


    def federated_dp_sgd(participant_features, participant_labels, gamma, n_iter,
                         obj_and_grad, theta_init, sigma, freq_obj_eval=10,
                         n_obj_eval=1000, random_state=None):
        """
        Simulates federated DP-SGD with n participants.

        Parameters similar to private_sgd, but operates over multiple participants.
        participant_features: dict of feature arrays for each participant.
        participant_labels: dict of label arrays for each participant.
        Other parameters: as defined before.

        Returns:
            theta: final model parameters.
            obj_list: list of objective values recorded during training.
        """
        rng = np.random.RandomState(random_state)
        n_participants = len(participant_features)
        # Assume all participants share same feature dimension for simplicity
        p = theta_init.shape[0]

        theta = theta_init.copy()
        obj_list = []

        # Preselect evaluation indices for objective computation using central dataset (if available)
        # For simulation, we might use data from participant 0 for objective evaluation
        X_eval = participant_features[0]
        y_eval = participant_labels[0]
        n_eval = X_eval.shape[0]
        idx_eval = rng.randint(0, n_eval, n_obj_eval)

        for t in range(n_iter):
            if t % freq_obj_eval == 0:
                obj, _ = obj_and_grad(theta, X_eval[idx_eval, :], y_eval[idx_eval])
                obj_list.append(obj)

            # Each participant computes a noisy gradient
            gradients = []
            for i in range(n_participants):
                X_i = participant_features[i]
                y_i = participant_labels[i]

                # Sample a single data point for mini-batch size 1
                idx = rng.randint(0, X_i.shape[0])
                X_batch = X_i[idx:idx+1, :]
                y_batch = y_i[idx:idx+1]

                _, grad = obj_and_grad(theta, X_batch, y_batch)
                noise = rng.normal(loc=0.0, scale=sigma, size=grad.shape)
                noisy_grad = grad + noise
                gradients.append(noisy_grad)

            # Aggregator averages the gradients
            avg_grad = np.mean(gradients, axis=0)
            step_size = gamma(t) if callable(gamma) else gamma
            theta -= step_size * avg_grad

        return theta, obj_list


if __name__ == "__main__":
    main()
