"""
Auto-generated from notebooks/membership_inference_attacks.ipynb.

Generated by tools/convert_notebooks.py. Do not edit manually.
"""


def main():
    import torch
    from torch import nn
    from torch.optim import Adam
    import torch.nn.functional as F
    from torch.nn import CrossEntropyLoss
    from torch.utils.data import DataLoader

    from torchvision import transforms
    from torchvision.models import resnet18, mobilenet_v2
    from torchvision.datasets.cifar import CIFAR10

    from tqdm import trange, tqdm

    torch.manual_seed(0)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    device


    norm_mean = (0.4914, 0.4822, 0.4465)
    norm_std = (0.2023, 0.1994, 0.2010)
    batch_size = 128

    mu = torch.tensor(norm_mean).view(3,1,1).to(device)
    std = torch.tensor(norm_std).view(3,1,1).to(device)

    upper_limit = ((1 - mu) / std)
    lower_limit = ((0 - mu) / std)

    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(norm_mean, norm_std),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(norm_mean, norm_std),
    ])

    trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

    testset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

    classes = ('plane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')


    def train_step(model, dataloader, loss_fn, optimizer):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = model(inputs)
            loss = loss_fn(outputs, labels)

            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        loss = running_loss / len(dataloader)
        accuracy = 100 * correct / total
        return loss, accuracy


    def train_teacher(model, n_epochs, loader=trainloader, temp=1):
        optimizer = Adam(model.parameters(), lr=0.001)
        loss_fn = CrossEntropyLoss()

        for epoch in range(n_epochs):
            epoch_loss, epoch_accuracy = train_step(model, loader, loss_fn, optimizer)

            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")


    from torchvision.models import ResNet18_Weights

    teacher = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1).to(device)
    teacher.fc = nn.Linear(teacher.fc.in_features, 10).to(device)

    train_teacher(teacher, 15)


    def test_clean(model, dataloader=testloader):
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in dataloader:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        accuracy = 100 * correct / total
        return accuracy


    print(f'Teacher Accuracy {test_clean(teacher):.2f}%')


    def distill(model, teacher, dataloader, optimizer, T, alpha=0.5):
        """
        Perform a distillation step with combined loss.

        Args:
            model: Student model.
            teacher: Teacher model.
            dataloader: DataLoader for training data.
            optimizer: Optimizer.
            T: Temperature.
            alpha: Weight for the classification loss.

        Returns:
            loss: Combined loss.
            accuracy: Accuracy on true labels.
        """
        model.train()
        teacher.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        loss_fn_ce = nn.CrossEntropyLoss()
        loss_fn_kd = nn.KLDivLoss(reduction='batchmean')

        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            # Forward pass
            with torch.no_grad():
                teacher_outputs = teacher(inputs)

            student_outputs = model(inputs)

            # Compute losses
            loss_ce = loss_fn_ce(student_outputs, labels)
            loss_kd = loss_fn_kd(
                F.log_softmax(student_outputs / T, dim=1),
                F.softmax(teacher_outputs / T, dim=1)
            ) * (T * T)

            # Combine losses
            loss = alpha * loss_ce + (1. - alpha) * loss_kd

            # Backward and optimize
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            # Compute accuracy
            _, predicted = torch.max(student_outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        loss = running_loss / len(dataloader)
        accuracy = 100 * correct / total
        return loss, accuracy


    def train_student(model, teacher, n_epochs, loader=trainloader, temp=100, alpha=0.5):
        optimizer = Adam(model.parameters(), lr=0.001)

        for epoch in range(n_epochs):
            epoch_loss, epoch_accuracy = distill(model, teacher, loader, optimizer, temp, alpha)

            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")


    student = resnet18(pretrained=False).to(device)
    student.fc = nn.Linear(student.fc.in_features, 10).to(device)

    train_student(student, teacher, 15, temp=100, alpha=0.5)


    print(f'Student Accuracy {test_clean(student):.2f}%')


    def attack_fgsm(model, x, y, epsilon):
        model.eval()
        x_adv = x.clone().detach().requires_grad_(True).to(device)

        outputs = model(x_adv)
        loss = CrossEntropyLoss()(outputs, y)

        model.zero_grad()
        loss.backward()

        data_grad = x_adv.grad.data
        sign_data_grad = data_grad.sign()

        perturbed_x = x_adv + epsilon * sign_data_grad

        perturbed_x = torch.max(torch.min(perturbed_x, upper_limit), lower_limit).detach()

        return perturbed_x


    def attack_pgd(model, x, y, epsilon, alpha=0.2, num_iters=10):
        model.eval()

        x_adv = x.clone().detach().to(device)

        for i in range(num_iters):
            x_adv.requires_grad = True

            outputs = model(x_adv)
            loss = CrossEntropyLoss()(outputs, y)

            model.zero_grad()
            loss.backward()

            data_grad = x_adv.grad.data
            sign_data_grad = data_grad.sign()

            x_adv = x_adv + alpha * sign_data_grad

            x_adv = torch.clamp(x_adv, x - epsilon, x + epsilon)

            x_adv = torch.max(torch.min(x_adv, upper_limit), lower_limit).detach()

        return x_adv


    def test_attack(model, epsilon, attack=attack_fgsm, loader=testloader):
        model.eval()
        correct = 0
        total = 0

        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)

            perturbed_inputs = attack(model, inputs, labels, epsilon)

            outputs = model(perturbed_inputs)

            _, predicted = torch.max(outputs.data, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        robust_accuracy = 100 * correct / total
        return robust_accuracy


    epsilons = [1, 2, 4, 8, 16]

    print("Robust Accuracy of the Teacher Model:")
    for eps in epsilons:
        acc_fgsm = test_attack(teacher, epsilon=eps/255, attack=attack_fgsm, loader=testloader)
        print(f'FGSM with ϵ={eps}/255 has Accuracy: {acc_fgsm:.2f}%')

        acc_pgd = test_attack(teacher, epsilon=eps/255, attack=attack_pgd, loader=testloader)
        print(f'PGD  with ϵ={eps}/255 has Accuracy: {acc_pgd:.2f}%')


    epsilons = [1, 2, 4, 8, 16]

    print("Robust Accuracy of the Student Model:")
    for eps in epsilons:
        acc_fgsm = test_attack(student, epsilon=eps/255, attack=attack_fgsm, loader=testloader)
        print(f'FGSM with ϵ={eps}/255 has Accuracy: {acc_fgsm:.2f}%')

        acc_pgd = test_attack(student, epsilon=eps/255, attack=attack_pgd, loader=testloader)
        print(f'PGD  with ϵ={eps}/255 has Accuracy: {acc_pgd:.2f}%')


    import matplotlib.pyplot as plt

    epsilons = [1, 2, 4, 8, 16]

    teacher_fgsm = [77.74, 71.97, 61.08, 42.36, 21.34]
    teacher_pgd = [77.68, 71.49, 59.79, 39.25, 18.49]

    student_fgsm = [71.13, 67.85, 61.08, 48.62, 29.78]
    student_pgd = [71.07, 67.70, 60.44, 46.22, 25.46]

    plt.figure(figsize=(10, 6))
    plt.plot(epsilons, teacher_fgsm, marker='o', label='Teacher FGSM')
    plt.plot(epsilons, teacher_pgd, marker='o', label='Teacher PGD')
    plt.plot(epsilons, student_fgsm, marker='s', label='Student FGSM')
    plt.plot(epsilons, student_pgd, marker='s', label='Student PGD')

    plt.title('Robust Accuracy vs Epsilon for Teacher and Student Models')
    plt.xlabel('Epsilon (ϵ)/255')
    plt.ylabel('Robust Accuracy (%)')
    plt.legend()
    plt.grid(True)
    plt.xticks(epsilons)
    plt.show()


    model = resnet18(weights=None).to(device)
    model.fc = nn.Linear(model.fc.in_features, 10).to(device)

    train_teacher(model, n_epochs=15, temp=1)


    surrogate_accuracy = test_clean(model)
    print(f'Surrogate Accuracy {surrogate_accuracy:.2f}%')


    epsilons = [1, 2, 4, 8, 16]

    print("Robust Accuracy of the Surrogate Model:")
    for eps in epsilons:
        acc_fgsm = test_attack(model, epsilon=eps/255, attack=attack_fgsm, loader=testloader)
        print(f'FGSM with ϵ={eps}/255 has Accuracy: {acc_fgsm:.2f}%')

        acc_pgd = test_attack(model, epsilon=eps/255, attack=attack_pgd, loader=testloader)
        print(f'PGD  with ϵ={eps}/255 has Accuracy: {acc_pgd:.2f}%')


    def transfer_attack(oracle, model, eps, loader=testloader):
        oracle.eval()
        model.eval()

        correct = 0
        total = 0

        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)

            perturbed_inputs = attack_fgsm(model, inputs, labels, eps)

            outputs = oracle(perturbed_inputs)
            _, predicted = torch.max(outputs.data, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        robust_accuracy = 100 * correct / total
        return robust_accuracy


    for eps in epsilons:
        acc = transfer_attack(student, model, eps/255)
        print(f'FGSM with ϵ={eps}/255 has Accuracy: {acc:.2f}%')


    def nes_gradient_estimate(model, x, y, epsilon, num_samples, sigma):
        model.eval()
        batch_size = x.shape[0]
        device = x.device

        grad_estimates = torch.zeros_like(x).to(device)

        for _ in range(num_samples):
            noise = torch.randn_like(x).to(device) * sigma
            x_pos = torch.clamp(x + noise, lower_limit, upper_limit)
            x_neg = torch.clamp(x - noise, lower_limit, upper_limit)

            outputs_pos = model(x_pos)
            loss_pos = F.cross_entropy(outputs_pos, y)

            outputs_neg = model(x_neg)
            loss_neg = F.cross_entropy(outputs_neg, y)

            loss_diff = (loss_pos - loss_neg) / (2 * sigma)

            grad_estimates += loss_diff * noise

        grad_estimates /= num_samples

        return grad_estimates


    def partial_information_attack(model, x, y, epsilon, num_samples, sigma, num_steps, alpha):

        model.eval()

        adv_x = x.clone().detach().to(device)

        for step in range(num_steps):
            grad_est = nes_gradient_estimate(model, adv_x, y, epsilon, num_samples, sigma)

            adv_x = adv_x + alpha * grad_est.sign()

            adv_x = torch.clamp(adv_x, x - epsilon, x + epsilon)

            adv_x = torch.max(torch.min(adv_x, upper_limit), lower_limit).detach()

        return adv_x


    from torch.utils.data import Subset

    def run_zoo_attack(oracle, surrogate, epsilon, num_samples, sigma, num_steps, alpha, loader=testloader, subset_size=100):
        oracle.eval()
        surrogate.eval()

        subset_indices = list(range(subset_size))
        subset = Subset(loader.dataset, subset_indices)
        subset_loader = DataLoader(subset, batch_size=32, shuffle=False)

        correct = 0
        total = 0

        for inputs, labels in tqdm(subset_loader, desc="ZOO Attack Progress"):
            inputs, labels = inputs.to(device), labels.to(device)

            adv_inputs = partial_information_attack(
                model=surrogate,
                x=inputs,
                y=labels,
                epsilon=epsilon,
                num_samples=num_samples,
                sigma=sigma,
                num_steps=num_steps,
                alpha=alpha
            )

            with torch.no_grad():
                outputs = oracle(adv_inputs)
                _, predicted = torch.max(outputs.data, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        robust_accuracy = 100 * correct / total
        return robust_accuracy

    epsilon = 8/255
    num_samples = 100
    sigma = 0.01
    num_steps = 10
    alpha = 0.2
    subset_size = 100

    robust_accuracy_zoo = run_zoo_attack(
        oracle=student,
        surrogate=model,
        epsilon=epsilon,
        num_samples=num_samples,
        sigma=sigma,
        num_steps=num_steps,
        alpha=alpha,
        loader=testloader,
        subset_size=subset_size
    )

    print(f'ZOO Attack with ϵ={epsilon*255}/255 on {subset_size} samples has Oracle Accuracy: {robust_accuracy_zoo:.2f}%')


    from robustbench.utils import load_model

    teacher = load_model(model_name='Gowal2021Improving_R18_ddpm_100m', dataset='cifar10', threat_model='Linf')


    def ard(student, teacher, dataloader, optimizer, eps, attack):
        student.train()
        teacher.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        loss_fn = nn.KLDivLoss(reduction='batchmean')

        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            adv_inputs = attack(student, inputs, labels, eps)

            with torch.no_grad():
                teacher_outputs = teacher(inputs)
            student_outputs = student(adv_inputs)

            soft_labels = F.log_softmax(student_outputs, dim=1)
            teacher_soft = F.softmax(teacher_outputs, dim=1)

            loss = loss_fn(soft_labels, teacher_soft)

            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            _, predicted = torch.max(student_outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        avg_loss = running_loss / len(dataloader)
        avg_accuracy = 100 * correct / total
        return avg_loss, avg_accuracy

    def adv_train_student(model, teacher, n_epochs, eps=8/255, loader=trainloader):
        optimizer = Adam(model.parameters(), lr=0.001)

        for epoch in range(n_epochs):
            epoch_loss, epoch_accuracy = ard(model, teacher, loader, optimizer, eps, attack=attack_fgsm)

            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")


    student = mobilenet_v2(weights=None).to(device)

    student.classifier[1] = nn.Linear(student.classifier[1].in_features, 10).to(device)

    adv_train_student(student, teacher, n_epochs=15, eps=8/255, loader=trainloader)


    def test_clean(model, dataloader=testloader):
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in dataloader:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        accuracy = 100 * correct / total
        return accuracy

    clean_accuracy = test_clean(student)
    print(f'Student Clean Accuracy: {clean_accuracy:.2f}%')

    fgsm_acc = test_attack(student, epsilon=8/255, attack=attack_fgsm, loader=testloader)
    print(f'Student FGSM with ϵ=8/255 has Accuracy: {fgsm_acc:.2f}%')

    pgd_acc = test_attack(student, epsilon=8/255, attack=attack_pgd, loader=testloader)
    print(f'Student PGD  with ϵ=8/255 has Accuracy: {pgd_acc:.2f}%')


if __name__ == "__main__":
    main()
