"""
Auto-generated from notebooks/adversarial_attacks_and_defenses.ipynb.

Generated by tools/convert_notebooks.py. Do not edit manually.
"""


def main():
    import torch
    import torchvision
    import torchvision.transforms as transforms
    from torchvision.models import resnet18
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    import numpy as np
    from torch.autograd import Variable
    from tqdm import tqdm


    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    print(device)


    # Define batch size and transformations
    batch_size = 128
    transform = transforms.Compose([transforms.ToTensor()])

    # Load CIFAR-10 training and test datasets
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)

    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')


    # Define a custom ResNet model
    class ResNet(nn.Module):
        def __init__(self, num_cls):
            super().__init__()
            self.conv = nn.Sequential(*list(resnet18(weights=None).children())[:-2])
            self.fc = nn.Linear(512, num_cls)

        def forward(self, x):
            x = self.conv(x)
            x = torch.flatten(x, start_dim=1)
            logits = self.fc(x)
            return logits

    # Initialize the model and move to device
    model = ResNet(num_cls=10).to(device)


    learning_rate = 0.01
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
    epochs = 100


    ##############################################################################
    # Implement train loop and log training loss after each epoch                #
    ##############################################################################
    # Training the model
    for epoch in range(epochs):
        running_loss = 0.0
        for images, labels in trainloader:
            images, labels = images.to(device), labels.to(device)

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        # Log training loss
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(trainloader):.4f}")
    ################################ End #########################################


    ##############################################################################
    # Evaluate the trained model on test set and print test accuracy             #
    ##############################################################################
    # Evaluate model on test dataset
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Test Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')################################ End #########################################


    # Save model checkpoint
    model_name = "resnet18_cifar10_model_pretrained.pth"
    model_PATH = f"{model_name}"
    torch.save(model.state_dict(), model_PATH)
    print(f"Model saved to {model_PATH}")


    ##############################################################################
    # run untargeted FGSM attack for epsilon = 1/255 and report its accuracy     #
    ##############################################################################
    # FGSM Attack for different epsilon values using torchattacks library
    import torchattacks

    epsilon_values = [1/255, 4/255, 8/255]
    for epsilon in epsilon_values:
        fgsm_attack = torchattacks.FGSM(model, eps=epsilon)

        correct = 0
        total = 0
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            adv_images = fgsm_attack(images, labels)
            outputs = model(adv_images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        print(f"FGSM Attack with epsilon {epsilon}: Test Accuracy = {100 * correct / total:.2f}%")
    ################################ End #########################################


    # PGD Attack from scratch
    class LinfPGDAttack(object):
        def __init__(self, model, epsilon=8/255, k=2, alpha=2/255):
            self.model = model
            self.epsilon = epsilon
            self.steps = k
            self.alpha = alpha

        def __call__(self, image, label):
            perturbed_image = image.clone().detach().to(device)
            perturbed_image.requires_grad = True

            for _ in range(self.steps):
                outputs = self.model(perturbed_image)
                loss = criterion(outputs, label)
                loss.backward()

                # Update perturbed image with alpha step in the direction of the gradient
                perturbed_image = perturbed_image + self.alpha * perturbed_image.grad.sign()
                perturbed_image = torch.clamp(perturbed_image, image - self.epsilon, image + self.epsilon).detach_()
                perturbed_image.requires_grad = True

            return perturbed_image


    ##############################################################################
    # Evaluate PGD attack on the trained model with k=2,4,8                      #
    ##############################################################################

    k_list = [2, 4, 8]
    epsilon = 8/255  # Perturbation size
    alpha = 2/255    # Step size

    # Ensure the model is in evaluation mode
    model.eval()

    for k in k_list:
        pgd_attack = LinfPGDAttack(model, epsilon=epsilon, k=k, alpha=alpha)
        correct = 0
        total = 0
        print(f"Evaluating PGD attack with k={k}")
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            # Apply PGD attack
            adv_images = pgd_attack(images, labels)
            # Predict with adversarial images
            outputs = model(adv_images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        accuracy = 100 * correct / total
        print(f'PGD Attack with k={k}: Test Accuracy = {accuracy:.2f}%')


    class UAPAttack(object):
        def __init__(self, model, epsilon=8/255, delta=2/255, max_iters=10, data_loader=None):
            """
            Initializes the UAPAttack.

            Parameters:
                model (nn.Module): The target model to attack.
                epsilon (float): The maximum perturbation.
                delta (float): The step size for each iteration.
                max_iters (int): Number of iterations to perform.
                data_loader (DataLoader): DataLoader for training data to generate UAP.
            """
            self.model = model
            self.epsilon = epsilon
            self.delta = delta
            self.max_iters = max_iters
            self.data_loader = data_loader

            # Initialize universal perturbation with requires_grad=True
            self.uap = torch.zeros((1, 3, 32, 32)).to(device)
            self.uap.requires_grad = True

            # Optimizer to update the universal perturbation
            self.optimizer = optim.Adam([self.uap], lr=self.delta)

        def generate_uap(self):
            """
            Generates the Universal Adversarial Perturbation (UAP).

            Returns:
                torch.Tensor: The generated UAP.
            """
            self.model.eval()  # Set model to evaluation mode

            for itr in range(self.max_iters):
                print(f"UAP Generation Iteration {itr+1}/{self.max_iters}")
                for images, labels in tqdm(self.data_loader, desc=f"Iteration {itr+1}"):
                    images, labels = images.to(device), labels.to(device)

                    # Zero the gradients
                    self.optimizer.zero_grad()

                    # Apply current perturbation and clamp to [0,1]
                    perturbed_images = torch.clamp(images + self.uap, 0, 1)

                    # Forward pass
                    outputs = self.model(perturbed_images)
                    loss = nn.CrossEntropyLoss()(outputs, labels)

                    # Backward pass to compute gradients w.r.t uap
                    loss.backward()

                    # Update the perturbation
                    self.optimizer.step()

                    # Project the perturbation to the epsilon ball
                    with torch.no_grad():
                        self.uap = torch.clamp(self.uap, -self.epsilon, self.epsilon)
                        self.uap.requires_grad = True  # Re-enable gradient computation

            return self.uap.detach()

        def __call__(self, images):
            """
            Applies the universal adversarial perturbation to the given images.

            Parameters:
                images (torch.Tensor): Batch of images to perturb.

            Returns:
                torch.Tensor: Perturbed images.
            """
            return torch.clamp(images + self.uap, 0, 1)


    ##############################################################################
    # Evaluate UAP attack on the trained model with max_iters = 10               #
    ##############################################################################

    max_iters = 10
    epsilon = 8/255
    delta = 0.01  # Step size for perturbation update

    # Initialize the UAP attack
    uap_attack = UAPAttack(model, epsilon=epsilon, delta=delta, max_iters=max_iters, data_loader=trainloader)
    print("Generating Universal Adversarial Perturbation (UAP)...")
    uap = uap_attack.generate_uap()

    # Evaluate the model on the test set with UAP applied
    correct = 0
    total = 0
    print("Evaluating UAP attack on the test set...")
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        # Apply UAP to the images
        adv_images = uap_attack(images)
        # Predict with adversarial images
        outputs = model(adv_images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f'UAP Attack with max_iters={max_iters}: Test Accuracy = {accuracy:.2f}%')


    ##############################################################################
    # Implement the function described                                           #
    ##############################################################################

    import matplotlib.pyplot as plt
    import numpy as np

    def visualize_attack(original_images, perturbed_images, original_labels, perturbed_labels):
        """
        Plots the original and perturbed images along with their predicted labels.

        Parameters:
            original_images (Tensor): Batch of original images.
            perturbed_images (Tensor): Batch of perturbed images.
            original_labels (Tensor): Predicted labels for original images.
            perturbed_labels (Tensor): Predicted labels for perturbed images.
        """
        batch_size = original_images.size(0)
        for i in range(batch_size):
            orig_img = original_images[i].cpu().detach().numpy().transpose(1, 2, 0)
            pert_img = perturbed_images[i].cpu().detach().numpy().transpose(1, 2, 0)
            orig_label = classes[original_labels[i]]
            pert_label = classes[perturbed_labels[i]]

            fig, axes = plt.subplots(1, 2, figsize=(8, 4))
            axes[0].imshow(orig_img)
            axes[0].set_title(f"Original: {orig_label}")
            axes[0].axis('off')

            axes[1].imshow(pert_img)
            axes[1].set_title(f"Perturbed: {pert_label}")
            axes[1].axis('off')

            plt.tight_layout()
            plt.show()


    ##############################################################################
    # Choose 3 random images from testset and run FGSM attack and visualize      #
    ##############################################################################

    import random

    # Get a batch of test images
    dataiter = iter(testloader)
    images, labels = dataiter._next_data()
    images, labels = images.to(device), labels.to(device)

    # Select 3 random indices
    indices = random.sample(range(images.size(0)), 3)
    selected_images = images[indices]
    selected_labels = labels[indices]

    # Run FGSM attack
    epsilon = 8/255
    fgsm_attack = torchattacks.FGSM(model, eps=epsilon)
    adv_images = fgsm_attack(selected_images, selected_labels)

    # Get predictions
    model.eval()
    with torch.no_grad():
        outputs_orig = model(selected_images)
        outputs_adv = model(adv_images)
        _, preds_orig = torch.max(outputs_orig, 1)
        _, preds_adv = torch.max(outputs_adv, 1)

    # Visualize the results
    visualize_attack(selected_images, adv_images, preds_orig, preds_adv)


    ##############################################################################
    # Choose 3 random images from testset and run PGD attack and visualize       #
    ##############################################################################

    # Reusing the same selected_images and selected_labels from above

    # Run PGD attack
    k = 4
    pgd_attack = LinfPGDAttack(model, epsilon=8/255, k=k, alpha=2/255)
    adv_images = pgd_attack(selected_images, selected_labels)

    # Get predictions
    model.eval()
    with torch.no_grad():
        outputs_orig = model(selected_images)
        outputs_adv = model(adv_images)
        _, preds_orig = torch.max(outputs_orig, 1)
        _, preds_adv = torch.max(outputs_adv, 1)

    # Visualize the results
    visualize_attack(selected_images, adv_images, preds_orig, preds_adv)


    ##############################################################################
    # Choose 3 random images from testset and run UAP attack and visualize       #
    ##############################################################################

    # Ensure that the UAP has been generated
    if not hasattr(uap_attack, 'uap'):
        print("Generating UAP first...")
        uap_attack.generate_uap()

    # Apply UAP attack
    adv_images = uap_attack(selected_images)

    # Get predictions
    model.eval()
    with torch.no_grad():
        outputs_orig = model(selected_images)
        outputs_adv = model(adv_images)
        _, preds_orig = torch.max(outputs_orig, 1)
        _, preds_adv = torch.max(outputs_adv, 1)

    # Visualize the results
    visualize_attack(selected_images, adv_images, preds_orig, preds_adv)


    ##############################################################################
    # Load a new resnet model for adversarial training                           #
    # Train the model on perturbed images from                                   #
    # untargeted FGSM attack with epsilon = 8/255 on train dataset               #
    # Also use the following setup for training the model                        #
    ##############################################################################

    # Define a custom ResNet model (same architecture as before)
    class ResNetAdv(nn.Module):
        def __init__(self, num_cls):
            super().__init__()
            self.conv = nn.Sequential(*list(resnet18(weights=None).children())[:-2])
            self.fc = nn.Linear(512, num_cls)

        def forward(self, x):
            x = self.conv(x)
            x = torch.flatten(x, start_dim=1)
            logits = self.fc(x)
            return logits

    # Initialize the adversarial model and move to device
    adv_model = ResNetAdv(num_cls=10).to(device)

    # Training setup
    learning_rate = 0.01
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(adv_model.parameters(), lr=learning_rate, momentum=0.9)
    epochs = 100


    ##############################################################################
    # Train the model on perturbed images from                                   #
    # untargeted FGSM attack with epsilon = 8/255 on train dataset               #
    # Also use the following setup for training the model                        #
    ##############################################################################

    import torchattacks

    # Define FGSM attack with epsilon=8/255
    fgsm_train = torchattacks.FGSM(adv_model, eps=8/255)

    # Training loop with adversarial examples
    for epoch in range(epochs):
        adv_model.train()
        running_loss = 0.0
        for images, labels in tqdm(trainloader, desc=f"Epoch {epoch+1}/{epochs}"):
            images, labels = images.to(device), labels.to(device)

            # Generate adversarial examples
            adv_images = fgsm_train(images, labels)

            # Forward pass on adversarial images
            outputs = adv_model(adv_images)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(trainloader)
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

        # Optional: Evaluate on test set every 10 epochs to monitor progress
        if (epoch + 1) % 10 == 0:
            adv_model.eval()
            correct = 0
            total = 0
            with torch.no_grad():
                for test_images, test_labels in testloader:
                    test_images, test_labels = test_images.to(device), test_labels.to(device)
                    outputs = adv_model(test_images)
                    _, predicted = torch.max(outputs.data, 1)
                    total += test_labels.size(0)
                    correct += (predicted == test_labels).sum().item()
            test_accuracy = 100 * correct / total
            print(f"Test Accuracy after Epoch {epoch+1}: {test_accuracy:.2f}%")


    ##############################################################################
    # Evaluate the new model on the test dataset                                 #
    ##############################################################################

    # Set the model to evaluation mode
    adv_model.eval()

    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = adv_model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    test_accuracy = 100 * correct / total
    print(f'Adversarially Trained Model Test Accuracy on 10000 test images: {test_accuracy:.2f}%')


    ##############################################################################
    # Run FGSM attack on the adversarially trained model with epsilon = 8/255    #
    ##############################################################################

    # Define FGSM attack with epsilon=8/255
    fgsm_attack_adv = torchattacks.FGSM(adv_model, eps=8/255)

    # Evaluate FGSM attack on the adversarially trained model
    correct = 0
    total = 0
    adv_model.eval()
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        # Generate adversarial examples
        adv_images = fgsm_attack_adv(images, labels)
        # Predict with adversarial images
        outputs = adv_model(adv_images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    fgsm_accuracy_adv = 100 * correct / total
    print(f'Adversarially Trained Model Accuracy under FGSM Attack (epsilon=8/255): {fgsm_accuracy_adv:.2f}%')


    ##############################################################################
    #  Run PGD attack on the adversarially trained model with k=4                #
    ##############################################################################

    # Define PGD attack parameters
    epsilon = 8/255
    alpha = 2/255
    k = 4

    # Initialize PGD attack
    pgd_attack_adv = LinfPGDAttack(adv_model, epsilon=epsilon, k=k, alpha=alpha)

    # Evaluate PGD attack on the adversarially trained model
    correct = 0
    total = 0
    adv_model.eval()
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        # Generate adversarial examples using PGD
        adv_images = pgd_attack_adv(images, labels)
        # Predict with adversarial images
        outputs = adv_model(adv_images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    pgd_accuracy_adv = 100 * correct / total
    print(f'Adversarially Trained Model Accuracy under PGD Attack (k=4): {pgd_accuracy_adv:.2f}%')


    ##############################################################################
    #  Run UAP attack on the adversarially trained model with max_iters = 10     #
    ##############################################################################

    # Initialize the UAP attack
    uap_attack_adv = UAPAttack(adv_model, epsilon=8/255, delta=2/255, max_iters=10, data_loader=trainloader)
    print("Generating Universal Adversarial Perturbation (UAP) for adversarially trained model...")
    uap_adv = uap_attack_adv.generate_uap()

    # Evaluate UAP attack on the adversarially trained model
    correct = 0
    total = 0
    adv_model.eval()
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        # Apply UAP to the images
        adv_images = uap_attack_adv(images)
        # Predict with adversarial images
        outputs = adv_model(adv_images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    uap_accuracy_adv = 100 * correct / total
    print(f'Adversarially Trained Model Accuracy under UAP Attack (max_iters=10): {uap_accuracy_adv:.2f}%')


    class UAPAttack(object):
        def __init__(self, model, epsilon=8/255, delta=2/255, max_iters=10, data_loader=None):
            self.model = model
            self.epsilon = epsilon
            self.delta = delta
            self.max_iters = max_iters
            self.data_loader = data_loader

            # Initialize universal perturbation to zero
            self.uap = torch.zeros(3, 32, 32).to(device)

        def generate_uap(self):
            self.model.eval()
            for _ in range(self.max_iters):
                for images, labels in tqdm(self.data_loader, desc=f"UAP Iteration {_+1}/{self.max_iters}"):
                    images, labels = images.to(device), labels.to(device)
                    images.requires_grad = True

                    # Add the current universal perturbation to the images
                    perturbed_images = torch.clamp(images + self.uap, 0, 1)

                    outputs = self.model(perturbed_images)
                    loss = criterion(outputs, labels)
                    self.model.zero_grad()
                    loss.backward()
                    grad = images.grad.data

                    # Update the universal perturbation
                    self.uap += self.delta * torch.sign(grad.mean(dim=0))
                    self.uap = torch.clamp(self.uap, -self.epsilon, self.epsilon)

            return self.uap

        def __call__(self, image):
            """
            Apply the universal perturbation to a given image.
            """
            perturbed_image = torch.clamp(image + self.uap, 0, 1)
            return perturbed_image


    # Save adversarially trained model
    adv_model.eval()
    adv_model_name = "resnet18_cifar10_model_adversarially_trained.pth"
    adv_model_PATH = f"{adv_model_name}"
    torch.save(adv_model.state_dict(), adv_model_PATH)
    print(f"Adversarially Trained Model saved to {adv_model_PATH}")


    def visualize_attack(original_images, perturbed_images, original_labels, perturbed_labels):
        """
        Plots the original and perturbed images along with their predicted labels.

        Parameters:
            original_images (Tensor): Batch of original images.
            perturbed_images (Tensor): Batch of perturbed images.
            original_labels (Tensor): Predicted labels for original images.
            perturbed_labels (Tensor): Predicted labels for perturbed images.
        """
        batch_size = original_images.size(0)
        for i in range(batch_size):
            orig_img = original_images[i].cpu().detach().numpy().transpose(1, 2, 0)
            pert_img = perturbed_images[i].cpu().detach().numpy().transpose(1, 2, 0)
            orig_label = classes[original_labels[i]]
            pert_label = classes[perturbed_labels[i]]

            fig, axes = plt.subplots(1, 2, figsize=(8, 4))
            axes[0].imshow(orig_img)
            axes[0].set_title(f"Original: {orig_label}")
            axes[0].axis('off')

            axes[1].imshow(pert_img)
            axes[1].set_title(f"Perturbed: {pert_label}")
            axes[1].axis('off')

            plt.tight_layout()
            plt.show()


    ##############################################################################
    # Choose 3 random images from testset and run FGSM attack and visualize      #
    ##############################################################################

    import random
    import matplotlib.pyplot as plt  # Ensure matplotlib is imported
    import torchattacks  # Ensure torchattacks is imported

    # Define the visualize_attack function if not already defined
    def visualize_attack(original_images, perturbed_images, original_labels, perturbed_labels):
        """
        Plots the original and perturbed images along with their predicted labels.

        Parameters:
            original_images (Tensor): Batch of original images.
            perturbed_images (Tensor): Batch of perturbed images.
            original_labels (Tensor): Predicted labels for original images.
            perturbed_labels (Tensor): Predicted labels for perturbed images.
        """
        batch_size = original_images.size(0)
        for i in range(batch_size):
            orig_img = original_images[i].cpu().detach().numpy().transpose(1, 2, 0)
            pert_img = perturbed_images[i].cpu().detach().numpy().transpose(1, 2, 0)
            orig_label = classes[original_labels[i]]
            pert_label = classes[perturbed_labels[i]]

            fig, axes = plt.subplots(1, 2, figsize=(8, 4))
            axes[0].imshow(orig_img)
            axes[0].set_title(f"Original: {orig_label}")
            axes[0].axis('off')

            axes[1].imshow(pert_img)
            axes[1].set_title(f"Perturbed: {pert_label}")
            axes[1].axis('off')

            plt.tight_layout()
            plt.show()

    # Get a batch of test images
    dataiter = iter(testloader)
    images, labels = next(dataiter)  # Corrected: Use next(dataiter)
    images, labels = images.to(device), labels.to(device)

    # Select 3 random indices
    indices = random.sample(range(images.size(0)), 3)
    selected_images = images[indices]
    selected_labels = labels[indices]

    # Run FGSM attack on adversarially trained model
    fgsm_attack_adv = torchattacks.FGSM(adv_model, eps=8/255)
    adv_images = fgsm_attack_adv(selected_images, selected_labels)

    # Get predictions
    adv_model.eval()
    with torch.no_grad():
        outputs_orig = adv_model(selected_images)
        outputs_adv = adv_model(adv_images)
        _, preds_orig = torch.max(outputs_orig, 1)
        _, preds_adv = torch.max(outputs_adv, 1)

    # Visualize the results
    visualize_attack(selected_images, adv_images, preds_orig, preds_adv)


    ##############################################################################
    # Choose 3 random images from testset and run PGD attack and visualize      #
    ##############################################################################

    # Run PGD attack on adversarially trained model
    k = 4
    pgd_attack_adv = LinfPGDAttack(adv_model, epsilon=8/255, k=k, alpha=2/255)
    adv_images_pgd = pgd_attack_adv(selected_images, selected_labels)

    # Get predictions
    adv_model.eval()
    with torch.no_grad():
        outputs_orig = adv_model(selected_images)
        outputs_adv_pgd = adv_model(adv_images_pgd)
        _, preds_orig_pgd = torch.max(outputs_orig, 1)
        _, preds_adv_pgd = torch.max(outputs_adv_pgd, 1)

    # Visualize the results
    visualize_attack(selected_images, adv_images_pgd, preds_orig_pgd, preds_adv_pgd)


    ##############################################################################
    # Choose 3 random images from testset and run UAP attack and visualize       #
    ##############################################################################

    # Ensure that the UAP for adversarially trained model has been generated
    if not hasattr(uap_attack_adv, 'uap') or uap_attack_adv.uap is None:
        print("Generating UAP for adversarially trained model...")
        uap_attack_adv.generate_uap()

    # Apply UAP attack
    adv_images_uap = uap_attack_adv(selected_images)

    # Get predictions
    adv_model.eval()
    with torch.no_grad():
        outputs_orig_uap = adv_model(selected_images)
        outputs_adv_uap = adv_model(adv_images_uap)
        _, preds_orig_uap = torch.max(outputs_orig_uap, 1)
        _, preds_adv_uap = torch.max(outputs_adv_uap, 1)

    # Visualize the results
    visualize_attack(selected_images, adv_images_uap, preds_orig_uap, preds_adv_uap)


if __name__ == "__main__":
    main()
